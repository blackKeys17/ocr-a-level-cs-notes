***
## Section (a)
### RISC processors
- RISC stands for **reduced instruction set**
- Each instruction in **assembly** in these processors corresponds to around 1 line of **machine code** and takes **1 clock cycle**
- This means that assembly code is *"lower level"* in RISC architectures
- This also means that compilers/interpreters need to do **more work** to translate high-level code into assembly
- Programs need to be **longer** as each line of assembly *can't do as much*

### CISC processors
- CISC stands for **complex instruction set**
- There is a **larger** instruction set, and individual instructions can take **multiple clock cycles**
	- *Multiplication is a good example of this*
- These instructions are built into the processor **hardware**, which needs to be more specialised
- This allows for the assembly code to be *closer* to high-level code
- An instruction in assembly can *do more*, so less **RAM** is required as programs are **shorter**

### Comparison of RISC and CISC

| RISC                                                                                                               | CISC                                                                                               |
| ------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------- |
| Compiler has to do **more work** to translate high level code into machine code                                    | Compiler can do **less work** to translate high level code into machine code                       |
| **More RAM** required to store code                                                                                | **Less RAM** required to store code                                                                |
| **Pipelining** is **possible** as instructions only take **1 clock cycle**                                         | **Pipelining** is **not effective** as instructions can take **multiple clock cycles**             |
| **Simpler** instructions, **not as many** instructions                                                             | **More complex** instructions, a lot of **specialised** instructions, which might **not get used** |
| **Less complex circuitry**, so processors can be **smaller** and consume **less power** and generate **less heat** | **More complex circuitry**                                                                         |
| Can be good for **mobile devices**                                                                                 | Typically used in **embedded systems**                                                             |
## Section (b)
### GPUs
- GPU stands for **graphics processing unit**
- It is responsible for processing **graphics** to reduce the load on the **CPU**
- GPUs contain a lot of smaller cores, which can process large amounts of data in **parallel**
- They are likely to have **built-in circuitry** for handling common graphics operations
- They are highly suitable for **SIMD (single instruction multiple data)** processing
	- This is when the **same instruction** is performed over **multiple** data points simultaneously
- GPUs are a type of **co-processor** (secondary processor designed to supplement the activities of a primary processor)
	
#### Applications of GPUs
- GPUs can also be used for:
##### 3D modelling
- Rendering lighting, textures and shadows
##### Financial modelling
- Simulating scenarios such as options pricing and other financial modelling situations
- Simulations can be run in parallel
##### Data mining
- Analysing large amounts of data to find patterns
##### Machine learning
- Operations such as matrix multiplication are particularly common
- More recently used to train LLMs (large language models)

## Section (c)
- **Parallel processing** is when a computer has multiple cores
- Each core can work on the **same task**, or they can work on **separate tasks** simultaneously
### Multicore processing
- Multicore systems has **more than 1 processing unit** which are capable of **independently** processing instructions
- For more speedup, each core can have its own **cache**
#### Speed
- If tasks can be divided into **subtasks** which can be completed simultaneously, total execution time can be **reduced**
- If part of a program *can't* be parallelised, there is a limit to what adding more cores can do
#### Performance
- There are **physical restrictions** from just speeding up 1 core
- This includes problems related to **heating**
- Computations can be done on different parts of a **large dataset** for example
#### Complexity
- It is **more difficult** to write code to be executed in parallel
- Tasks need to be **synchronised** and data needs to be **shared**
- It is more difficult to **debug** a parallel program than a **serial** program