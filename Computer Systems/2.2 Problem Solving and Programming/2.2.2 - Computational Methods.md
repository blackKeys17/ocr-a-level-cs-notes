***
## Section (a)

### Features that make a problem solvable by computational methods
- There are **real-world limits** which determine whether a problem is **solvable** using **computational methods**
- There are multiple considerations which need to be taken in to account:

#### Resources
- Can the problem be solved by an **algorithm** in a **realistic** amount of time?
- Is there enough **processing power**, **memory** and **storage** to solve the problem?
- **Advances in technology** are allowing us to solve much **larger** and **more complex** problems 

#### Suitability
- Generally, problems which consist of **inputs**, **outputs** and **calculations** are suitable to a computational approach
- Note that the definition of an **input/output/calculation** is pretty *flexible*
- **Ethical** or **social** problems, such as whether a loan should be approved, may require some **human input** instead of being completely automated

## Section (b)

### Problem recognition/identification
- It is important to **clearly** identify what the problem is
- **Stakeholders** may state the requirements which a finished product should have
- This may involve:
	- Analysing a **current** solution, and evaluating its **strengths** and **weaknesses**
	- Considering the **types of data** involved

#### Example - GP surgery booking system
- Patients are reporting **slow response times** for a GP surgery's online booking system
- They notice **large delays** when confirming appointments
- It is most noticeable at its peak hours

##### The main problem
- Slow response time
- This causes user frustration

##### Subproblems
- Significant slowdown at **peak hours**, which could indicate issues with **scalability**
- The booking requests are **inefficient** if the users need to wait several minutes
- This could affect the **reputation** of the GP, if a lot of its users are frustrated

## Section (c)

### Problem decomposition
- This was already shown *a bit* in the previous example
- This stage involves **breaking down** a problem into **smaller subproblems**, which can be divided again into **even smaller subproblems**
- This is usually done until each **subproblem** can be implemented by a single **subroutine**
- This is done to reduce the **complexity** of the problem

#### Evaluation
- It may be found that some of the subproblems can be dealt with by using **libraries** or **modules**, reducing the load on the developers
- It can make teams **easier to manage**, as different teams can be assigned **different subproblems** and work on the problem in **parallel**
- It can make **debugging** easier, as **errors can be identified** in individual modules and dealt with individually 

## Section (d)

### Divide and conquer
- This technique can be separated into **3 stages**:
	- Divide
	- Conquer
	- Merge
- In the **divide** stage, the problem is split into **smaller subproblems**
- In the **conquer** stage, the smallest subproblems are **solved individually**
- In the **merge** stage, the solutions to the subproblems are **recombined** together to form a final solution

#### Binary Search
- Binary search can be considered to be a **divide and conquer** algorithm
- However, this algorithm doesn't really **merge** solutions to subproblems, and instead *discards* them
- The **original problem** of finding a value in a sorted list can be reduced or **divided** by looking at the item in the middle of the list:
	- If the item is the target, then we are done
	- Otherwise, depending on whether the target is smaller or larger than this item, we can either choose to only look at the top half of the list or the bottom half
	- This can be repeated until either the target is found in the list, or we are left with a sublist of length 1, then it doesn't exist in the list

#### Merge sort and quicksort
- Both of these sorting algorithms are **divide and conquer**
- The problems of sorting the original list is **divided** into the task of sorting 2 sublists
- The original list is broken down recursively until the sublists are of length 1, which are trivially sorted (and **conquered**)
- The sorted sublists are **merged** together in both algorithms, until the original list has been sorted

#### Evaluation
- Very complex problems can be **simplified greatly** when their size is halved at every stage
- This means that a lot of divide and conquer algorithms are **fast**
- However, as they are usually implemented **recursively**, they can be difficult to **debug** and **trace** 

## Section (e)

### Abstraction
- This is when **excessive** or **unnecessary** details are **removed** or **ignored** to **simplify** a problem
- This allows developers to **focus** on the most **important** or **core aspects** of the problem which they are working on
- **Levels of abstraction** can be used, where code can be written using modules **without necessarily knowing** how they are implemented
- This make projects more **manageable**

- **Abstraction by generalisation** can be used to group different sections of the code with **similar underlying functionality**
- This allows for code to be **reused** for different parts of a program, which **saves time**

## Section (f)

### Backtracking
- In some problems, if we meet a **dead end**, we can **go back** (or *backtrack*) to a previous state
- **DFS (depth first search)** is an example of a backtracking algorithm
- When DFS is implemented **recursively**, this backtracking is **implicit**, as there is nothing in the code which explicitly tells the algorithm to backtrack 

- A way of thinking of backtracking is with a **maze**
	- We can try visiting each path
	- If we meet a dead end, we can **backtrack** to the last place where we had a choice

### Data mining
- Identifying **patterns** and **anomalies** in large sets of data **(big data)**
- This is used to **spot trends** and **anomalies** and **identify correlations** which *aren't* immediately obvious to a human
- It doesn't necessarily **explain** *why* these patterns exist
- Data mining requires a lot of computation and **processing power**
- The **big data** involved in data mining often involves **personal data**, so they way it is handled must follow the rules set by the **GDPR**
- The data collected also needs to be **stored** and **protected**

#### Techniques
- Clustering - **grouping similar data points together**, which could allow for patterns to be more easily identified
- Labelling - Adding informative labels to data, usually to help train a machine learning model

#### Example - shopping
- Companies can collect data about their customers' shopping habits
- This can be used to **predict** what people might start buying more in certain parts of the year
- This can help with **marketing** or **organising which items are displayed** in the context of a shop
- A famous example is Target, which predicted pregnancies purely based on customer data

### Heuristics
- Some problems are **really difficult to solve**
	- Usually this means that they can't be solved optimally within a **reasonable amount of time**
- Heuristics are **non-optimal** methods to solve a problem **faster** with less computations
- Usually, the solution found with a heuristic **isn't perfect**, but they are **good enough**
- Sometimes, there is a risk of a heuristic approach getting locked into a **suboptimal solution** or **local minimum**

#### Examples of using heuristics

##### Travelling salesman problem (TSP)
- Nearly intractable normally
- Can use optimisation algorithms and heuristics such as greedy algorithms to speed up

##### Pathfinding / Navigation
- **Dijkstra's algorithm** can be quite slow in practice
- The **A* algorithm** can be used instead, which introduces a heuristic which estimates the total distance left to a target node
- This means that it performs a lot better in practice

##### Machine learning
- ML models aren't designed to be completely perfect
- For example, designing a perfect classifier with brute force computation is completely infeasible
- Heuristics can be used along with ML as well, to determine whether a method which involves a lot more computation is really needed over a simpler heuristic

### Performance modelling
- Process of **predicting** how a system performs using a **model** using collected **data**
- Eliminates need for actual performance tests
- **Cheaper**, **less time-consuming** and **safer** to implement than actual tests

#### Examples
- For example, if we want to test a cloud server, instead of actually flooding it with a lot of requests, it can be made to operate with a lower load and have its response tracked closely
- On a plane, it isn't practical to do a real trial run of simulating an issue, but performance modelling can be used to safely test the software on the plane
- Can be used for estimating the runtime of a job, as it is pointless to wait and find out how long it actually takes
- Optimising database architecture and querying

### Pipelining
- More can be found [[1.1.1 - Structure and Function of the Processor#Pipelining|here]]
- Outside of the processor, pipelining can allow for projects to be **delivered faster** by having different tasks developed in **parallel**, instead of a *production line* approach where the output of a process becomes the input for another
- Make sure to identify processes which can be run in **parallel** and **sequentially** in questions

### Visualisation
- **Presenting data** in a way which is **easier for humans** to **understand**
- This could allow for easier **identification of trends**, especially with statistical data, which aren't **immediately obvious**
- For example, it is much easier to interpret a **bar graph** than a table containing a lot of numbers

#### Methods of visualisation
- Graphs
- Trees
- Charts
- Tables
