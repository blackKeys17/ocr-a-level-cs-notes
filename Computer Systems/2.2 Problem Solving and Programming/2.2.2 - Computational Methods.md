***
## Section (a)

### Features that make a problem solvable by computational methods
- There are **real-world limits** which determine whether a problem is **solvable** using **computational methods**
- There are multiple considerations which need to be taken in to account:

#### Resources
- Can the problem be solved by an **algorithm** in a **realistic** amount of time?
- Is there enough **processing power**, **memory** and **storage** to solve the problem?
- **Advances in technology** are allowing us to solve much **larger** and **more complex** problems 

#### Suitability
- Generally, problems which consist of **inputs**, **outputs** and **calculations** are suitable to a computational approach
- Note that the definition of an **input/output/calculation** is pretty *flexible*
- **Ethical** or **social** problems, such as whether a loan should be approved, may require some **human input** instead of being completely automated

## Section (b)

### Problem recognition/identification
- It is important to **clearly** identify what the problem is
- **Stakeholders** may state the requirements which a finished product should have
- This may involve:
	- Analysing a **current** solution, and evaluating its **strengths** and **weaknesses**
	- Considering the **types of data** involved

#### Example - GP surgery booking system
- Patients are reporting **slow response times** for a GP surgery's online booking system
- They notice **large delays** when confirming appointments
- It is most noticeable at its peak hours
##### The main problem
- Slow response time
- This causes user frustration
##### Subproblems
- Significant slowdown at **peak hours**, which could indicate issues with **scalability**
- The booking requests are **inefficient** if the users need to wait several minutes
- This could affect the **reputation** of the GP, if a lot of its users are frustrated

## Section (c)

### Problem decomposition
- This was already shown *a bit* in the previous example
- This stage involves **breaking down** a problem into **smaller subproblems**, which can be divided again into **even smaller subproblems**
- This is usually done until each **subproblem** can be implemented by a single **subroutine**
- This is done to reduce the **complexity** of the problem

#### Evaluation
- It may be found that some of the subproblems can be dealt with by using **libraries** or **modules**, reducing the load on the developers
- It can make teams **easier to manage**, as different teams can be assigned **different subproblems** and work on the problem in **parallel**
- It can make **debugging** easier, as **errors can be identified** in individual modules and dealt with individually 

## Section (d)

### Divide and conquer
- This technique can be separated into **3 stages**:
	- Divide
	- Conquer
	- Merge
- In the **divide** stage, the problem is split into **smaller subproblems**
- In the **conquer** stage, the smallest subproblems are **solved individually**
- In the **merge** stage, the solutions to the subproblems are **recombined** together to form a final solution

#### Binary Search
- Binary search can be considered to be a **divide and conquer** algorithm
- However, this algorithm doesn't really **merge** solutions to subproblems, and instead *discards* them
- The **original problem** of finding a value in a sorted list can be reduced or **divided** by looking at the item in the middle of the list:
	- If the item is the target, then we are done
	- Otherwise, depending on whether the target is smaller or larger than this item, we can either choose to only look at the top half of the list or the bottom half
	- This can be repeated until either the target is found in the list, or we are left with a sublist of length 1, then it doesn't exist in the list

#### Merge sort and quicksort
- Both of these sorting algorithms are **divide and conquer**
- The problems of sorting the original list is **divided** into the task of sorting 2 sublists
- The original list is broken down recursively until the sublists are of length 1, which are trivially sorted (and **conquered**)
- The sorted sublists are **merged** together in both algorithms, until the original list has been sorted

#### Evaluation
- Very complex problems can be **simplified greatly** when their size is halved at every stage
- This means that a lot of divide and conquer algorithms are **fast**
- However, as they are usually implemented **recursively**, they can be difficult to **debug** and **trace** 

## Section (e)

### Abstraction
- This is when **excessive** or **unnecessary** details are **removed** or **ignored** to **simplify** a problem
- This allows developers to **focus** on the most **important** or **core aspects** of the problem which they are working on
- **Levels of abstraction** can be used, where code can be written using modules **without necessarily knowing** how they are implemented
- This make projects more **manageable**

- **Abstraction by generalisation** can be used to group different sections of the code with **similar underlying functionality**
- This allows for code to be **reused** for different parts of a program, which **saves time**

## Section (f)

### Backtracking
- In some problems, if we meet a **dead end**, we can **go back** (or *backtrack*) to a previous state
- **DFS (depth first search)** is an example of a backtracking algorithm
- When DFS is implemented **recursively**, this backtracking is **implicit**, so there is nothing in the code which explicitly tells the algorithm to backtrack 

- A way of thinking of backtracking is with a **maze**
	- We can try visiting each path
	- If we meet a dead end, we can **backtrack** to the last place where we had a choice

### Data mining
- Identifying **patterns** and **anomalies** in large sets of data **(big data)**
- This is used to **spot trends** and **identify correlations** which *aren't* immediately obvious to a human
- The **big data** involved in data mining often involves **personal data**, so they way it is handled must follow the rules set by the **GDPR**

#### Example - shopping
- Companies can collect data about their customers' shopping habits
- This can be used to **predict** what people might start buying more in certain parts of the year
- A famous example is Target, which predicted pregnancies purely based on customer data
